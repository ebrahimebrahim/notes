{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction is often neglected in ML courses, but\n",
    "it is also often one of the main bottlenecks in the ML pipeline\n",
    "\n",
    "A _feature template_ is a group of features all computed in the same way.\n",
    "Basically a feature description with blanks in it.\n",
    "\n",
    "e.g.\n",
    "- \"string length is greater than [blank]\"\n",
    "- \"string contains the character [blank]\"\n",
    "\n",
    "\n",
    "# Hypothesis Class\n",
    "\n",
    "Suppose you have a particular form of predictor in mind, \n",
    "like linear classifier $ f_w(x) = w^T \\phi(x) $.\n",
    "\n",
    "Now you are trying to pick a feature extraction funciton $\\phi$.\n",
    "\n",
    "One thing to consider is the _hypothesis class_ induced by your choice of $\\phi$.\n",
    "\n",
    "The hypothesis class corresponding to $\\phi$ is the set $\\{f_w\\ |\\ w\\in\\mathbb{R}^d\\}$, where $d$ is the number of features.\n",
    "\n",
    "Note that linear classifiers can produce crazy nonlinear decision boundaries, all depending on how $\\phi$ is chosen.\n",
    "\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "They are basically like linear predictors strung together with activation functions. Each layer solves a bunch of sub-problems. The output of each layer is like features for the next layer.\n",
    "\n",
    "Taking gradient of loss function can be done in a systematic way as follows:\n",
    "- Create computation graph for expression, where nodes are building blocks like $+$ and $\\cdot$ and $\\sigma$, etc.\n",
    "- Label edges with partial derivatives of the building blocks wrt their inputs.\n",
    "- Observe that the chain rule is now simply multiplication of edges along paths. In other words if you want to know the deriv of one node value wrt one of its descendents, just multiply edge values along the path joining them.\n",
    "- Define forward values, $f_i$'s, to be the values on the nodes-- i.e. the values of subexpressions. Define backward values, $g_i$'s, to be the derivatives $\\frac{\\partial \\text{output}}{\\partial f_i}$ -- i.e. how changing that node influces the root.\n",
    "- Backpropagation algorithm:\n",
    "    - Forward pass: Compute each $f_i$, leaves to root. So basically just compute the top level expression and make sure to remember the values of all subexpressions along the way\n",
    "    - Backward pass: Compute each $g_i$, root to leaves. This is just a matter of traveling down paths to the nodes you're interested in taking a deriv wrt, and along the way multiplying the derivative expressions that you labeled on edges, which you can now compute because you have all the $f_i$'s in place.\n",
    "\n",
    "# Nearest Neighbors\n",
    "\n",
    "Store your _entire_ training data set. Given a new input $x$, find the data point in your training set whose input is nearest to $x$. The associated output is then your prediction. That's it! With spatial input and euclidean distance, this creates Voronoi diagrams.\n",
    "\n",
    "No pre-defined set of parameters-- each new data point is its own parameter. Model complexity scales with the size of the data set. This situation is referred to as a _non-parametric model_.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "- Linear predictors are fast, easy to learn, but have weak use of features.\n",
    "- Neural nets are fast, hard to learn, and have powerful use of features.\n",
    "- Nearest neighbor is slow, easy to learn, and has powerful use of features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
