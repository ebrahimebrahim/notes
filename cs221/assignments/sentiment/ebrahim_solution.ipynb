{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scl(s,v):\n",
    "    \"\"\"return s*v where is a scalar and v is a sparse vector\"\"\"\n",
    "    return {key:s*val for key,val in v.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pretty': 1, 'bad': 1},\n",
       " {'good': 1, 'plot': 1},\n",
       " {'not': 1, 'good': 1},\n",
       " {'pretty': 1, 'scenery': 1}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = [\n",
    "            \"pretty bad\",\n",
    "            \"good plot\",\n",
    "            \"not good\",\n",
    "            \"pretty scenery\",\n",
    "          ]\n",
    "\n",
    "def phi(review):\n",
    "    \"\"\"Returns sparse feature vector from review\"\"\"\n",
    "    return {word:1 for word in review.split()}\n",
    "\n",
    "xs = list(map(phi,reviews))\n",
    "ys = [-1.,1.,-1.,1.] # ground truth\n",
    "\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_hinge(x,y,w):\n",
    "    return max(0,1-y * util.dotProduct(w,x))\n",
    "\n",
    "def training_loss(xs,ys,w):\n",
    "    return np.array([loss_hinge(x,y,w) for x,y in zip(xs,ys)]).mean()\n",
    "\n",
    "def loss_hinge_grad(x,y,w):\n",
    "    if 1 - (y*util.dotProduct(w,x)) <= 0 : return 0\n",
    "    else: return scl(-y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: {}, training loss: 1.0\n",
      "w: {'pretty': -0.5, 'bad': -0.5}, training loss: 0.875\n",
      "w: {'pretty': -0.5, 'bad': -0.5, 'good': 0.5, 'plot': 0.5}, training loss: 0.75\n",
      "w: {'pretty': -0.5, 'bad': -0.5, 'good': 0.0, 'plot': 0.5, 'not': -0.5}, training loss: 0.625\n",
      "w: {'pretty': 0.0, 'bad': -0.5, 'good': 0.0, 'plot': 0.5, 'not': -0.5, 'scenery': 0.5}, training loss: 0.5\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "w = {}\n",
    "for x,y in zip(xs,ys):\n",
    "    print(\"w: {}, training loss: {}\".format(w, training_loss(xs,ys,w)))\n",
    "    util.increment(w, -eta, loss_hinge_grad(x,y,w))\n",
    "print(\"w: {}, training loss: {}\".format(w, training_loss(xs,ys,w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last vector is the answer to problem 1e.\n",
    "\n",
    "It is possible to reach training loss 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: {}, training loss: 1.0\n",
      "w: {'pretty': -0.5, 'bad': -0.5}, training loss: 0.875\n",
      "w: {'pretty': -0.5, 'bad': -0.5, 'good': 0.5, 'plot': 0.5}, training loss: 0.75\n",
      "w: {'pretty': -0.5, 'bad': -0.5, 'good': 0.0, 'plot': 0.5, 'not': -0.5}, training loss: 0.625\n",
      "w: {'pretty': 0.0, 'bad': -0.5, 'good': 0.0, 'plot': 0.5, 'not': -0.5, 'scenery': 0.5}, training loss: 0.5\n",
      "w: {'pretty': -0.5, 'bad': -1.0, 'good': 0.0, 'plot': 0.5, 'not': -0.5, 'scenery': 0.5}, training loss: 0.5\n",
      "w: {'pretty': -0.5, 'bad': -1.0, 'good': 0.5, 'plot': 1.0, 'not': -0.5, 'scenery': 0.5}, training loss: 0.5\n",
      "w: {'pretty': -0.5, 'bad': -1.0, 'good': 0.0, 'plot': 1.0, 'not': -1.0, 'scenery': 0.5}, training loss: 0.25\n",
      "w: {'pretty': 0.0, 'bad': -1.0, 'good': 0.0, 'plot': 1.0, 'not': -1.0, 'scenery': 1.0}, training loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "w = {}\n",
    "for _ in range(2):\n",
    "    for x,y in zip(xs,ys):\n",
    "        print(\"w: {}, training loss: {}\".format(w, training_loss(xs,ys,w)))\n",
    "        util.increment(w, -eta, loss_hinge_grad(x,y,w))\n",
    "print(\"w: {}, training loss: {}\".format(w, training_loss(xs,ys,w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1f\n",
    "\n",
    "Suppose we had a dataset consisting of the reviews\n",
    "- $r_1 = $\"good\"\n",
    "- $r_2=$\"not good\"\n",
    "- $r_3=$\"bad\"\n",
    "- $r_4=$\"not bad\"\n",
    "\n",
    "The ground truth about these reviews is obvious: $[+1,-1,-1,+1]$\n",
    "(where $+1$ means it's a positive review and $-1$ means negative review).\n",
    "\n",
    "Then the feature space of word counts has dimension 3, if we say\n",
    "$$ \\phi([\\text{a review}]) = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\text{number of occurances of \"not\"}\\\\\n",
    "\\text{number of occurances of \"good\"}\\\\\n",
    "\\text{number of occurances of \"bad\"}\n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "**Claim:** It is impossible for a linear classifier to get zero error on the dataset above.\n",
    "\n",
    "_Proof:_ Suppose we had a linear classifier given by weight vector\n",
    "$w$, and that it gets zero error on the training set above. Remember that the prediction\n",
    "for input review $r$ would be $\\operatorname{sign}(w\\cdot\\phi(r))$.\n",
    "Then we'd have\n",
    "- $w_2 > 0$\n",
    "- $w_1 + w_2 < 0$\n",
    "- $w_3<0$\n",
    "- $w_1+w_3>0$\n",
    "\n",
    "From the first two we have\n",
    "$$ w_1 < -w_2 < 0, $$\n",
    "and from the last two we have\n",
    "$$ w_1 > -w_3 > 0, $$\n",
    "a contradiction. $\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "(To cut down on notation I write $x$ where I should really write $\\phi(x)$)\n",
    "\n",
    "(a)\n",
    "$\\text{Loss}(x,y,w)=(\\sigma(w\\cdot x) - y)^2$\n",
    "\n",
    "(b)\n",
    "$\\nabla_w \\text{Loss} = 2(\\sigma(w\\cdot x)-y)\\sigma(w\\cdot x)(1-\\sigma(w\\cdot x)) x$\n",
    "\n",
    "or if we write $p=\\sigma(w\\cdot x)$ then it's\n",
    "$\\nabla_w \\text{Loss} = 2(p-y)p(1-p) x$\n",
    "\n",
    "(c) Suppose we make the vector $w$ be $\\frac{\\alpha}{||x||^2} x $, where $\\alpha$ is a very large positive number.\n",
    "Then $w\\cdot x$ is $\\alpha$ and so $p= \\sigma(w\\cdot x) = \\sigma(\\alpha)\\approx 1$. If $y$ is $\\pm 1$ then this makes $\\nabla_w \\text{Loss}$ very close to $0$. This is the vanishing gradient problem.\n",
    "\n",
    "(d) Assume $y$ is $1$. Then\n",
    "$$\n",
    "\\begin{align*}\n",
    "|| \\nabla_w \\text{Loss} || \n",
    "&= 2 |(p-1)p(1-p)|  \\, ||x||\\\\\n",
    "&= 2 p (1-p)^2  \\, ||x||\n",
    "\\end{align*}\n",
    "$$\n",
    "Using basic calculus to optimize $p (1-p)^2$ over the domain $p\\in (0,1)$ (i.e. the range of the sigmoid),\n",
    "we get $p=\\frac{1}{3}$ as the maximizing value. There certainly exists $w$ that makes $\\sigma(w\\cdot x)$ be $\\frac{1}{3}$. Hence the best bound we can get is\n",
    "$$\n",
    "\\begin{align*}\n",
    "|| \\nabla_w \\text{Loss} || \n",
    "&\\leq 2 \\frac{1}{3} (1-\\frac{1}{3})^2  \\, ||x||\\\\\n",
    "&=\\frac{8}{27}||x||\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(e) Set $y_n'=\\sigma^{-1}(y_n)$ and choose $w^*=w$... if I understood the problem correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
