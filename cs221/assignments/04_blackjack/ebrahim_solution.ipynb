{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "## 1a\n",
    "\n",
    "$\\newcommand{Vopt}[2]{V_\\text{opt}^{(#1)}(#2)}$\n",
    "We initialize $\\Vopt{0}{s}=0$ for all $s$.\n",
    "Then we apply value iteration:\n",
    "\n",
    "$$\n",
    "\\Vopt{t+1}{s} = \\max_{a\\in\\{-1,+1\\}} \\sum_{s'\\in\\{-2,\\ldots,2\\}} T(s,a,s')(\\text{Reward}(s,a,s')+\\Vopt{t}{s'})\n",
    "$$\n",
    "\n",
    "Here $\\gamma=1$ and for simplicity I am not writing that $\\Vopt{t}{s}=0$ for any end state $s$, i.e. for $s\\in\\{-2,2\\}$.\n",
    "\n",
    "Now to determine $\\Vopt{1}{s}$.\n",
    "\n",
    "- $\\Vopt{1}{-1}$ is the expected utility of being in state $s=-1$ and applying the optimal policy.\n",
    "    - The optimal policy would choose either $a=-1$ or $a=+1$ depending on which yields a greater expected utility. First consider playing $a=-1$.\n",
    "        - We'd have a $T(-1,-1,-2)=0.8$ chance of earning $r=20$ and reaching state $s'=-2$, \n",
    "        - and we'd have a $T(-1,-1,0)=0.2$ chance of earning $r=-5$ and reaching state $s'=0$.\n",
    "        - Thus the expected value of playing $a=-1$ \n",
    "          is \n",
    "          $$Q_\\text{opt}^{(1)}(-1,-1) = 0.8 (20 + \\Vopt{0}{-2}) + 0.2 (-5 + \\Vopt{0}{0}) = 0.8(20)+0.2(-5) = 15$$\n",
    "    - Now we consider playing $a=+1$.\n",
    "        - We'd have a $T(-1,+1,-2)=0.7$ chance of earning $r=20$ and reaching state $s'=-2$, \n",
    "        - and we'd have a $T(-1,+1,0)=0.3$ chance of earning $r=-5$ and reaching state $s'=0$.\n",
    "        - Thus the expected value of playing $a=+1$ \n",
    "          is \n",
    "          $$Q_\\text{opt}^{(1)}(-1,+1) = 0.7 (20 + \\Vopt{0}{-2}) + 0.3 (-5 + \\Vopt{0}{0}) = 0.7(20)+0.3(-5) = 12.5$$\n",
    "    - The optimal policy (according to what we think so far; remember we are biased by our initially crappy $V_\\text{opt}$ estimate) would therefore play $a=-1$ when at state $s=-1$, which\n",
    "      makes $\\Vopt{1}{-1} = Q_\\text{opt}^{(1)}(s=-1,a=-1) = 15$\n",
    "- $\\Vopt{1}{0}$\n",
    "    - expected value of playing $a=-1$ is\n",
    "      $$0.8 (-5 + \\Vopt{0}{-1}) + 0.2 (-5 + \\Vopt{0}{1}) = 0.8(-5)+0.2(-5) = -5$$\n",
    "    - expected value of $a=+1$ is\n",
    "      $$0.7 (-5 + \\Vopt{0}{-1}) + 0.3 (-5 + \\Vopt{0}{1}) = 0.7(-5)+0.3(-5) = -5$$\n",
    "    - $a=-1$ and $a=+1$ are tied when $s=-1$. The max between $Q_\\text{opt}^{(1)}(s=0,a=-1)$ and $Q_\\text{opt}^{(1)}(s=0,a=+1)$ is $-5$. So\n",
    "      $\\Vopt{1}{0} = -5$.\n",
    "- $\\Vopt{1}{1}$\n",
    "    - expected value of playing $a=-1$ is\n",
    "      $$0.8 (-5 + \\Vopt{0}{0}) + 0.2 (100 + \\Vopt{0}{2}) = 0.8(-5)+0.2(100) = 16$$\n",
    "    - expected value of $a=+1$ is\n",
    "      $$0.7 (-5 + \\Vopt{0}{0}) + 0.3 (100 + \\Vopt{0}{2}) = 0.7(-5)+0.3(100) = 26.5$$\n",
    "    - $a=+1$ is optimal\n",
    "      $\\Vopt{1}{1} = Q_\\text{opt}^{(1)}(s=1,a=+1) = 26.5$.     \n",
    "\n",
    "We ended up with $\\Vopt{1}{s}$ being $0,15,-5,26.5,0$ for $s=-2,1,0,1,2$ respectively.\n",
    "\n",
    "Another iteration, let's determine $\\Vopt{2}{s}$.\n",
    "\n",
    "- $\\Vopt{2}{-1}$\n",
    "    - expected value of playing $a=-1$ is\n",
    "      $$0.8 (20 + \\Vopt{1}{-2}) + 0.2 (-5 + \\Vopt{1}{0}) = 0.8(20)+0.2(-5-5) = 14$$\n",
    "    - expected value of $a=+1$ is\n",
    "      $$0.7 (20 + \\Vopt{1}{-2}) + 0.3 (-5 + \\Vopt{1}{0}) = 0.7(20)+0.3(-5-5) = 11$$\n",
    "    - $\\Vopt{2}{-1} = 14$.\n",
    "- $\\Vopt{2}{0}$\n",
    "    - expected value of playing $a=-1$ is\n",
    "      $$0.8 (-5 + \\Vopt{1}{-1}) + 0.2 (-5 + \\Vopt{1}{1}) = 0.8(-5+15)+0.2(-5+26.5) = 12.3$$\n",
    "    - expected value of $a=+1$ is\n",
    "      $$0.7 (-5 + \\Vopt{1}{-1}) + 0.3 (-5 + \\Vopt{1}{1}) = 0.7(-5+15)+0.3(-5+26.5) = 13.45$$\n",
    "    - $\\Vopt{2}{0} = 13.45$.\n",
    "- $\\Vopt{2}{1}$\n",
    "    - expected value of playing $a=-1$ is\n",
    "      $$0.8 (-5 + \\Vopt{1}{0}) + 0.2 (100 + \\Vopt{1}{2}) = 0.8(-5-5)+0.2(100) = 12$$\n",
    "    - expected value of $a=+1$ is\n",
    "      $$0.7 (-5 + \\Vopt{1}{0}) + 0.3 (100 + \\Vopt{1}{2}) = 0.7(-5-5)+0.3(100) = 23$$\n",
    "    - $\\Vopt{2}{1} = 23$.\n",
    "\n",
    "We ended up with $\\Vopt{2}{s}$ being $0,14,13.45,23,0$ for $s=-2,1,0,1,2$ respectively.\n",
    "\n",
    "## 1b\n",
    "\n",
    "Let's do one more iteration, but this time just focusing on the $Q$s so that we can determine the optimal policy.\n",
    "\n",
    "- From $s=-1$\n",
    "  - $Q_\\text{opt}^{(3)}(s=-1,a=-1) = 0.8 (20 + \\Vopt{2}{-2}) + 0.2 (-5 + \\Vopt{2}{0}) $ \n",
    "  `= 0.8*(20)+0.2*(-5+13.45) = 17.69`\n",
    "  - $Q_\\text{opt}^{(3)}(s=-1,a=1) = 0.7 (20 + \\Vopt{2}{-2}) + 0.3 (-5 + \\Vopt{2}{0}) $ \n",
    "  `= 0.7*(20)+0.3*(-5+13.45) = 16.535`\n",
    "- From $s=0$\n",
    "  - $Q_\\text{opt}^{(3)}(s=0,a=-1) = 0.8 (-5 + \\Vopt{2}{-1}) + 0.2 (-5 + \\Vopt{2}{1}) $ \n",
    "  `= 0.8*(-5 + 14)+0.2*(-5+23) = 10.8`\n",
    "  - $Q_\\text{opt}^{(3)}(s=0,a=1) = 0.7 (-5 + \\Vopt{2}{-1}) + 0.3 (-5 + \\Vopt{2}{1}) $ \n",
    "  `= 0.7*(-5+14)+0.3*(-5+23) = 11.7`\n",
    "- From $s=1$\n",
    "  - $Q_\\text{opt}^{(3)}(s=1,a=-1) = 0.8 (-5 + \\Vopt{2}{0}) + 0.2 (100 + \\Vopt{2}{2}) $ \n",
    "  `= 0.8*(-5+13.45)+0.2*(100) = 26.76`\n",
    "  - $Q_\\text{opt}^{(3)}(s=1,a=1) = 0.7 (-5 + \\Vopt{2}{0}) + 0.3 (100 + \\Vopt{2}{2}) $ \n",
    "  `= 0.7*(-5+13.45)+0.3*(100) = 35.915`\n",
    "\n",
    "So here's our optimal policy based on the estimate $\\Vopt{2}{s}$ of $V_\\text{opt}(s)$:\n",
    "- $\\pi(-1) = -1$\n",
    "- $\\pi(0)  = +1$\n",
    "- $\\pi(1)  = +1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "## 2a\n",
    "Testing that my solution in submission.py works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from submission import *\n",
    "\n",
    "vi = ValueIteration()\n",
    "vi.solve(CounterexampleMDP())\n",
    "vi.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueIteration: 2 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'happy': 0, 'sad': 0, 'begin': -49.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This class is not written by me, it is copied from grader.py\n",
    "class AddNoiseMDP(util.MDP):\n",
    "    def __init__(self, originalMDP):\n",
    "        self.originalMDP = originalMDP\n",
    "\n",
    "    def startState(self):\n",
    "        return self.originalMDP.startState()\n",
    "\n",
    "    # Return set of actions possible from |state|.\n",
    "    def actions(self, state):\n",
    "        return self.originalMDP.actions(state)\n",
    "\n",
    "    # Return a list of (newState, prob, reward) tuples corresponding to edges\n",
    "    # coming out of |state|.\n",
    "    def succAndProbReward(self, state, action):\n",
    "        originalSuccAndProbReward = self.originalMDP.succAndProbReward(state, action)\n",
    "        newSuccAndProbReward = []\n",
    "        for state, prob, reward in originalSuccAndProbReward:\n",
    "            newProb = 0.5 * prob + 0.5 / len(originalSuccAndProbReward)\n",
    "            newSuccAndProbReward.append((state, newProb, reward))\n",
    "        return newSuccAndProbReward\n",
    "\n",
    "    # Return set of actions possible from |state|.\n",
    "    def discount(self):\n",
    "        return self.originalMDP.discount()\n",
    "\n",
    "    \n",
    "    \n",
    "vi = ValueIteration()\n",
    "vi.solve(AddNoiseMDP(CounterexampleMDP()))\n",
    "vi.V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(indeed the value of \"begin\" state has increased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b\n",
    "If the MDP graph is acyclic then we should be able to apply the recurrence directly in an algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&V_\\text{opt}(s) = 0\\hspace{2em} \\text{if $s$ is an end state, otherwise:}\\\\\n",
    "&V_\\text{opt}(s) = \\max_{a\\in\\text{Actions}(s)} \\sum_{s'\\in \\text{States}\\\\T(s,a,s')\\neq 0} T(s,a,s')(\\text{Reward}(s,a,s')+\\gamma V_\\text{opt}(s'))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The acyclicity condition guarantees that this terminates; note that we only consider terms in the sum for which $T(s,a,s')$ is nonzero.\n",
    "\n",
    "We can use memoization: cache the already-computed $V_\\text{opt}(s)$ values. I think this guarantees that we won't pass over any $(s,a,s')$ triple more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c\n",
    "\n",
    "For short I will write $R$ for $\\text{Reward}$ and $V$ for $V_\\text{opt}$. I'll use $V'$ for $V'_\\text{opt}$, the optimal policy value function for the new MDP. The goal is to define $T'$ and $R'$ so that $V'=V$ on states where both are defined (i.e. states other than $o$).\n",
    "\n",
    "\n",
    "\n",
    "We will make the new state $o$ an end state, so that $V'(o)=0$. There will never be a reward for going to the state $o$, so $R'(s,a,o)=0$ for all $s\\in\\text{States}$ and $a\\in\\text{Actions}(s)$.\n",
    "\n",
    "For $s,s'\\in\\text{States}$ and $a\\in\\text{Actions}(s)$, we define\n",
    "\n",
    "$$\\begin{align*}\n",
    "T'(s,a,s') &= \\gamma \\,T(s,a,s')\\\\\n",
    "R'(s,a,s') &= \\tfrac{1}{\\gamma} R(s,a,s')\n",
    "\\end{align*}$$\n",
    "\n",
    "We've now arranged things so that for $s,s'\\in\\text{States}$ and $a\\in\\text{Actions}(s)$,\n",
    "\n",
    "$$\\begin{align*}\n",
    "T'(s,a,s')(R'(s,a,s') + v) = T(s,a,s')(R(s,a,s') + \\gamma v)\n",
    "\\end{align*}$$\n",
    "\n",
    "for any real number $v$.\n",
    "\n",
    "We still have to define $T'(s,a,o)$, but this is forced on us by the requirement that the $T$'s be probabilities:\n",
    "\n",
    "$$\\begin{align*}\n",
    "T'(s,a,o) = 1-\\gamma\n",
    "\\end{align*}$$\n",
    "\n",
    "for all $s\\in\\text{States}$ and $a\\in\\text{Actions}(s)$.\n",
    "\n",
    "Now the recursion relation that $V'$ satisfies is\n",
    "\n",
    "$$\\begin{align*}\n",
    "V'(s) &= 0\\hspace{2em} \\text{if $s$ is an end state, otherwise:}\\\\\n",
    "V'(s) &= \\max_{a\\in\\text{Actions}(s)} \\sum_{s'\\in \\text{States'}} T'(s,a,s')(R'(s,a,s')+V'(s'))\\\\\n",
    "&= \\max_{a\\in\\text{Actions}(s)} \\left[\\left(\\sum_{s'\\in \\text{States}} T(s,a,s')(R(s,a,s')+\\gamma V'(s'))\\right) + T'(s,a,o)(0+0)\\right]\\\\\n",
    "&= \\max_{a\\in\\text{Actions}(s)} \\sum_{s'\\in \\text{States}} T(s,a,s')(R(s,a,s')+\\gamma V'(s'))\n",
    "\\end{align*}$$\n",
    "\n",
    "This is exactly the same recursion relation that $V(s)$ satisfies, and so $V'(s)=V(s)$.\n",
    "\n",
    "---\n",
    "\n",
    "It's interesting that discounting the future by factor $\\gamma$ is equivalent to having a probability $1-\\gamma$ of terminating at each action. If it weren't for death, humans probably would value future rewards more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3a\n",
    "\n",
    "Testing out the `BlackjackMDP` I came up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((7, None, (0, 1)), 0.5, 0), ((11, None, None), 0.5, 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp1 = BlackjackMDP(cardValues=[1, 5], multiplicity=2,\n",
    "                                   threshold=10, peekCost=1)\n",
    "preBustState = (6, None, (1, 1))\n",
    "\n",
    "mdp1.succAndProbReward(preBustState,'Take')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueIteration: 11 iterations\n",
      "Peek is chosen for 13.793103448275861 percent of the states.\n"
     ]
    }
   ],
   "source": [
    "vi = ValueIteration()\n",
    "vi.solve(BlackjackMDP(\n",
    "        cardValues = [3,4,18],\n",
    "        multiplicity = 3,\n",
    "        threshold = 20,\n",
    "        peekCost = 1\n",
    "        ))\n",
    "#for key,val in vi.pi.items():\n",
    "#    print(\"{: >20}{: >7}\".format(str(key),str(val)))\n",
    "print(\"Peek is chosen for\",\n",
    "      100*float(len([_ for _,val in vi.pi.items() if val=='Peek']))/len(vi.pi),\n",
    "      \"percent of the states.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import submission\n",
    "import util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def simulate_QL_over_MDP(mdp, featureExtractor,explorationProb,verbose=True):\n",
    "\n",
    "    qlearner=submission.QLearningAlgorithm(\n",
    "      actions=lambda s : ['Take','Quit','Peek'] if s[1] is None else ['Take','Quit'],\n",
    "      discount=mdp.discount(), # Not really cheating I think\n",
    "      featureExtractor=featureExtractor,\n",
    "      explorationProb=explorationProb,\n",
    "    )\n",
    "    \n",
    "    # Learn a policy\n",
    "    rwds = util.simulate(\n",
    "                          mdp=mdp,\n",
    "                          rl=qlearner,\n",
    "                          numTrials=30000,\n",
    "                          maxIterations=10000,\n",
    "                          verbose=False\n",
    "                        )\n",
    "\n",
    "    qlearner_ave_rwd_exploration = np.array(rwds).mean()\n",
    "\n",
    "    # No more exploring, time to perform\n",
    "    qlearner.explorationProb = 0.0\n",
    "\n",
    "    # Perform\n",
    "    rwds = util.simulate(\n",
    "                          mdp=mdp,\n",
    "                          rl=qlearner,\n",
    "                          numTrials=30000,\n",
    "                          maxIterations=10000,\n",
    "                          verbose=False\n",
    "                        )\n",
    "\n",
    "    qlearner_ave_rwd = np.array(rwds).mean()\n",
    "\n",
    "    vi = util.ValueIteration()\n",
    "    vi.solve(mdp)\n",
    "    vopt_start = vi.V[mdp.startState()]\n",
    "\n",
    "    print(\n",
    "        \"Q Learner's average reward during exploration:\",\n",
    "        qlearner_ave_rwd_exploration,\n",
    "        \"\\nQ Learner's average reward after exploration:\",\n",
    "        qlearner_ave_rwd,\n",
    "        \"\\n Expected utility of true optimal policy:\",\n",
    "        vopt_start\n",
    "         )\n",
    "    \n",
    "    states_with_diff_actions = [s for s in mdp.states if vi.pi[s]!=qlearner.getAction(s)]\n",
    "    num_early_quits = len([s for s in mdp.states if vi.pi[s]!=qlearner.getAction(s) and qlearner.getAction(s) == \"Quit\"])\n",
    "    num_bad_takes   = len([s for s in mdp.states if vi.pi[s]!=qlearner.getAction(s) and qlearner.getAction(s) == \"Take\"])\n",
    "    \n",
    "    print(\n",
    "        len(states_with_diff_actions),\n",
    "        \"out of\",\n",
    "        len(mdp.states),\n",
    "        \"states led to non-optimal actions.\\n\",\n",
    "        num_early_quits,\n",
    "        \"of these are situations where the qlearner Quits before it should.\\n\",\n",
    "        num_bad_takes,\n",
    "        \"of them are situations where the qlearner Takes when it shouldn't.\"\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Here are all the nonoptimal actions:\")\n",
    "        for s in states_with_diff_actions:\n",
    "            print(\n",
    "                \"\\tIn state\",\n",
    "                s,\n",
    "                \"our qlearner chooses to\",\n",
    "                qlearner.getAction(s),\n",
    "                \"when the optimal choice is to\",\n",
    "                vi.pi[s]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueIteration: 5 iterations\n",
      "Q Learner's average reward during exploration: 4.6805 \n",
      "Q Learner's average reward after exploration: 6.827566666666667 \n",
      " Expected utility of true optimal policy: 6.833333333333333\n",
      "2 out of 27 states led to non-optimal actions.\n",
      " 2 of these are situations where the qlearner Quits before it should.\n",
      " 0 of them are situations where the qlearner Takes when it shouldn't.\n",
      "Here are all the nonoptimal actions:\n",
      "\tIn state (5, 1, (2, 1)) our qlearner chooses to Quit when the optimal choice is to Take\n",
      "\tIn state (5, 0, (2, 1)) our qlearner chooses to Quit when the optimal choice is to Take\n"
     ]
    }
   ],
   "source": [
    "simulate_QL_over_MDP(submission.smallMDP, submission.identityFeatureExtractor,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueIteration: 15 iterations\n",
      "Q Learner's average reward during exploration: 6.5995333333333335 \n",
      "Q Learner's average reward after exploration: 8.769266666666667 \n",
      " Expected utility of true optimal policy: 35.52569097569097\n",
      "946 out of 2745 states led to non-optimal actions.\n",
      " 228 of these are situations where the qlearner Quits before it should.\n",
      " 718 of them are situations where the qlearner Takes when it shouldn't.\n"
     ]
    }
   ],
   "source": [
    "simulate_QL_over_MDP(submission.largeMDP, submission.identityFeatureExtractor,0.3,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What went wrong in the case of the large MDP? My first thought is that the model complexity is too high and there are just too many (state,action) pairs that are never visited by the Q Learner.\n",
    "\n",
    "But it's not really that crazy... there are 2745 states and 3 actions, for a total of 8235 (state,action) pairs.\n",
    "Why would 200K episodes (which I tried) not be sufficient to explore them?\n",
    "Even the totally random exploration seems to not work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueIteration: 15 iterations\n",
      "Q Learner's average reward during exploration: 4.486 \n",
      "Q Learner's average reward after exploration: 11.3165 \n",
      " Expected utility of true optimal policy: 35.52569097569097\n",
      "1739 out of 2745 states led to non-optimal actions.\n",
      " 1334 of these are situations where the qlearner Quits before it should.\n",
      " 405 of them are situations where the qlearner Takes when it shouldn't.\n"
     ]
    }
   ],
   "source": [
    "simulate_QL_over_MDP(submission.largeMDP, submission.identityFeatureExtractor,1.0,verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c\n",
    "Now we try a lower dimensional hand-crafted feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueIteration: 5 iterations\n",
      "Q Learner's average reward during exploration: 4.839533333333334 \n",
      "Q Learner's average reward after exploration: 6.828966666666667 \n",
      " Expected utility of true optimal policy: 6.833333333333333\n",
      "5 out of 27 states led to non-optimal actions.\n",
      " 3 of these are situations where the qlearner Quits before it should.\n",
      " 0 of them are situations where the qlearner Takes when it shouldn't.\n",
      "Here are all the nonoptimal actions:\n",
      "\tIn state (10, None, None) our qlearner chooses to Quit when the optimal choice is to Take\n",
      "\tIn state (7, None, None) our qlearner chooses to Peek when the optimal choice is to Take\n",
      "\tIn state (6, 0, (1, 1)) our qlearner chooses to Quit when the optimal choice is to Take\n",
      "\tIn state (1, None, None) our qlearner chooses to Peek when the optimal choice is to Take\n",
      "\tIn state (6, None, None) our qlearner chooses to Quit when the optimal choice is to Take\n"
     ]
    }
   ],
   "source": [
    "simulate_QL_over_MDP(submission.smallMDP, submission.blackjackFeatureExtractor,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueIteration: 15 iterations\n",
      "Q Learner's average reward during exploration: 19.1484 \n",
      "Q Learner's average reward after exploration: 33.932966666666665 \n",
      " Expected utility of true optimal policy: 35.52569097569097\n",
      "539 out of 2745 states led to non-optimal actions.\n",
      " 521 of these are situations where the qlearner Quits before it should.\n",
      " 10 of them are situations where the qlearner Takes when it shouldn't.\n"
     ]
    }
   ],
   "source": [
    "simulate_QL_over_MDP(submission.largeMDP, submission.blackjackFeatureExtractor,0.3,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueIteration: 5 iterations\n",
      "ValueIteration: 5 iterations\n",
      "Expected reward from old policy: 6.8395\n",
      "Expected reward from true optimal policy: 12.0\n",
      "Now let's simulate with the q learner on the new policy...\n",
      "Expected reward from Q-learning behavior on new polocy: 9.360066666666667\n"
     ]
    }
   ],
   "source": [
    "import submission\n",
    "import util\n",
    "import numpy as np\n",
    "\n",
    "vi = util.ValueIteration()\n",
    "vi.solve(submission.originalMDP)\n",
    "vi2 = util.ValueIteration()\n",
    "vi2.solve(submission.newThresholdMDP)\n",
    "rwds = util.simulate(\n",
    "    mdp = submission.newThresholdMDP,\n",
    "    rl = util.FixedRLAlgorithm(vi.pi),\n",
    "    numTrials=30000\n",
    ")\n",
    "print(\"Expected reward from old policy: {}\".format(np.array(rwds).mean()))\n",
    "print(\"Expected reward from true optimal policy: {}\".format(vi2.V[submission.newThresholdMDP.startState()]))\n",
    "print(\"Now let's simulate with the q learner on the new policy...\")\n",
    "rwds = util.simulate(\n",
    "    mdp = submission.newThresholdMDP,\n",
    "    rl = submission.QLearningAlgorithm(\n",
    "      actions=lambda s : ['Take','Quit','Peek'] if s[1] is None else ['Take','Quit'],\n",
    "      discount=submission.newThresholdMDP.discount(), # Not really cheating I think\n",
    "      featureExtractor=submission.blackjackFeatureExtractor,\n",
    "    ),\n",
    "    numTrials=30000\n",
    ")\n",
    "print(\"Expected reward from Q-learning behavior on new polocy: {}\".format(np.array(rwds).mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
